{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!! trainingId:  0\n",
      "!!! loss:  0.09000000000000001\n",
      "impactOfA1OnLoss [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "!!! trainingId:  1\n",
      "!!! loss:  0.09014863625907263\n",
      "impactOfA1OnLoss [[ 3.53694491e-20]\n",
      " [ 3.53694491e-20]\n",
      " [ 3.53694491e-20]\n",
      " [ 3.53694491e-20]\n",
      " [ 3.53694491e-20]\n",
      " [-1.67058269e-19]\n",
      " [ 3.53694491e-20]\n",
      " [ 3.53694491e-20]\n",
      " [ 3.53694491e-20]\n",
      " [ 3.53694491e-20]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "alpha = 0.1;\n",
    "trainingId = 0;\n",
    "\n",
    "def sigmoid(x):\n",
    "    # https://en.wikipedia.org/wiki/Sigmoid_function\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    # https://www.geeksforgeeks.org/derivative-of-the-sigmoid-function/\n",
    "    return sigmoid(x) * (1.0 - sigmoid(x))\n",
    "\n",
    "def softmax(x):\n",
    "    # Subtract the maximum value in each row for numerical stability\n",
    "    x_max = np.max(x, axis=0, keepdims=True)\n",
    "    # Compute the exponentials\n",
    "    e_x = np.exp(x - x_max)\n",
    "    # Normalize the exponentials to get the probabilities\n",
    "    return e_x / np.sum(e_x, axis=0, keepdims=True)\n",
    "\n",
    "# softmaxExample_1Column_Of_10Digits = softmax([\n",
    "# [0],\n",
    "# [2],\n",
    "# [4],\n",
    "# [6],\n",
    "# [8],\n",
    "# [9],\n",
    "# [7],\n",
    "# [5],\n",
    "# [3],\n",
    "# [1]]);\n",
    "\n",
    "X = x_train[trainingId].reshape(-1,1)\n",
    "W1 = np.zeros((10, X.size));\n",
    "B1 = np.zeros((10, 1));\n",
    "Z1 = np.zeros((10, 1));\n",
    "A1 = np.zeros((10, 1));\n",
    "\n",
    "W2 = np.zeros((10, 10));\n",
    "B2 = np.zeros((10, 1));\n",
    "Z2 = np.zeros((10, 1));\n",
    "Y = np.zeros((10, 1));\n",
    "expectY = np.zeros((Y.size,1));\n",
    "expectY[y_train[trainingId]] = 1;\n",
    "\n",
    "def loadTrainingData():\n",
    "    global X, expectY;\n",
    "    X = x_train[trainingId].reshape(-1,1);\n",
    "    expectY = np.zeros((10, 1));\n",
    "    expectY[y_train[trainingId]] = 1;\n",
    "\n",
    "\n",
    "### Forward Propagation\n",
    "\n",
    "def forwardPropagationFromXToA1():\n",
    "    global Z1, A1;\n",
    "    Z1 = W1 @ X + B1;\n",
    "    A1 = sigmoid(Z1);\n",
    "\n",
    "def forwardPropagationFromA1ToY(): \n",
    "    global Z2, Y;\n",
    "    Z2 = W2 @ A1 + B2;\n",
    "    Y = softmax(Z2);\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "def loss():\n",
    "    global Y, expectY;\n",
    "    # Compute the MSE loss\n",
    "    return np.sum((expectY - Y) ** 2) / expectY.size\n",
    "\n",
    "\n",
    "### Backward propagation\n",
    "\n",
    "def impactOf_A1_On_Z2():\n",
    "    # Z2 = W2 @ A1 + B2;\n",
    "    # dZ2/dA1 = W2\n",
    "    return W2;\n",
    "\n",
    "def impactOf_Y_On_Loss():\n",
    "    # L = 1/Y.size * [(expectY1 - Y1)^2 + (expectY2 - Y2)^2 + ... + (expectYn - Yn)^2]\n",
    "    # dL/dYn = 2 / Y.size * (expectYn - Yn) * (-1)\n",
    "    return 2 / expectY.size * (expectY - Y) * (-1);\n",
    "\n",
    "def impactOf_Z2_On_Y():\n",
    "     # Y = softmax(Z2);\n",
    "     # softmax only changes the scale of the output, so the derivative is the same as the identity\n",
    "     # so no impact\n",
    "    return np.ones((Y.size, 1));\n",
    "\n",
    "def impactOf_Z1_On_A1():\n",
    "    # A1 = sigmoid(Z1);\n",
    "    # dA1/dZ1 = sigmoid_derivative(Z1)\n",
    "    return sigmoid_derivative(Z1);\n",
    "\n",
    "def impactOf_W2_On_Z2():\n",
    "    # Z2 = W2 @ A1 + B2;\n",
    "    # dZ2/dW2 = A1\n",
    "    return A1\n",
    "\n",
    "def impactOf_B2_On_Z2():\n",
    "    # Z2 = W2 @ A1 + B2;\n",
    "    # dZ2/dB2 = 1\n",
    "    return np.ones((Z2.size, 1));\n",
    "\n",
    "def impactOf_W2_On_Loss():\n",
    "    # chaining rule: dL/dW2 = dL/dY * dY/dZ2 * dZ2/dW2\n",
    "    return (impactOf_Y_On_Loss() * impactOf_Z2_On_Y()) @ impactOf_W2_On_Z2().T;\n",
    "\n",
    "def impactOf_W1_On_Z1():\n",
    "    # Z1 = W1 @ X + B1;\n",
    "    # dZ1/dW1 = X\n",
    "    return X;\n",
    "\n",
    "def impactOf_B1_On_Z1():\n",
    "    # Z1 = W1 @ X + B1;\n",
    "    # dZ1/dB1 = 1\n",
    "    return np.ones((Z1.size, 1));\n",
    "\n",
    "def impactOf_B2_On_Loss():\n",
    "    # chaining rule: dL/dB2 = dL/dY * dY/dZ2 * dZ2/dB2\n",
    "    return impactOf_Y_On_Loss() * impactOf_Z2_On_Y() * impactOf_B2_On_Z2();\n",
    "\n",
    "def impactOf_A1_On_Loss():\n",
    "    # chaining rule: dL/dA1 = dL/dY * dY/dZ2 * dZ2/dA1\n",
    "    return impactOf_A1_On_Z2() @ (impactOf_Y_On_Loss() * impactOf_Z2_On_Y());\n",
    "\n",
    "def impactOf_W1_On_Loss():\n",
    "    # chaining rule: dL/dW1 = dL/dA1 * dA1/dZ1 * dZ1/dW1\n",
    "    return (impactOf_A1_On_Loss() * impactOf_Z1_On_A1()) @ impactOf_W1_On_Z1().T;\n",
    "\n",
    "def impactOf_B1_On_Loss():\n",
    "    # chaining rule: dL/dB1 = dL/dA1 * dA1/dZ1 * dZ1/dB1\n",
    "    return impactOf_A1_On_Loss() * impactOf_Z1_On_A1() * impactOf_B1_On_Z1();\n",
    "\n",
    "def backPropagationFromYToA1():\n",
    "    global W2, B2;\n",
    "    W2 = W2 - alpha * impactOf_W2_On_Loss();\n",
    "    B2 = B2 - alpha * impactOf_B2_On_Loss();\n",
    "\n",
    "def backPropagationFromA1ToX():\n",
    "    global W1, B1    \n",
    "    W1 = W1 - alpha * impactOf_W1_On_Loss();\n",
    "    B1 = B1 - alpha * impactOf_B1_On_Loss();\n",
    "\n",
    "def train():\n",
    "    global trainingId;\n",
    "    for i in range(2):\n",
    "        trainingId = i;\n",
    "        loadTrainingData();\n",
    "        forwardPropagationFromXToA1();\n",
    "        forwardPropagationFromA1ToY();\n",
    "        print(\"!!! trainingId: \", trainingId);\n",
    "        print(\"!!! loss: \", loss());\n",
    "        print(\"impactOfA1OnLoss\", impactOf_A1_On_Loss());\n",
    "        backPropagationFromYToA1();\n",
    "        backPropagationFromA1ToX();\n",
    "\n",
    "train();\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
